# Team1_Berezin-Andrusiak

1. Обчислення Gini Impurity

def gini_impurity(counts, total): probabilities = counts / total return 1 - np.sum(probabilities**2) 

Мета: Обчислення критерію чистоти вузла дерева (Gini impurity), який використовується для оцінки якості розбиття.

• Вхідні дані: 

• counts: кількість зразків кожного класу в поточному вузлі.

• total: загальна кількість зразків у вузлі.

• Вихідні дані: Значення Gini impurity (від 0 до 1), де 0 означає ідеальну чистоту вузла.

2. Пошук найкращого розбиття

def best_split(X, y): ... 

Мета: Знайти оптимальну ознаку та поріг розбиття, щоб мінімізувати Gini impurity.

Логіка: 

• Ітерація по всіх ознаках (feature_index).

• Сортування зразків за значеннями поточної ознаки.

• Поступове перенесення зразків із правої частини до лівої, розрахунок Gini impurity для кожної частини.

• Оцінка зваженої суми Gini impurity для розбиття.

• Збереження найкращого розбиття, якщо воно зменшує impurity.

• Вихідні дані: 

• Індекс найкращої ознаки.

• Поріг значення, за яким розбиваються дані.

• Ліві та праві частини ознак і цільових значень.

3. Рекурсивна побудова дерева

def build_tree(X, y, max_depth=None, depth=0): ... 

• Мета: Побудова дерева рішень за допомогою рекурсивного розбиття даних.

• Логіка: 

• Базові умови зупинки: 

• Якщо всі зразки належать до одного класу.

• Якщо досягнуто максимальну глибину (max_depth).

• Виклик функції best_split для поточного вузла.

• Рекурсивна побудова лівого та правого піддерев.

• Вихідні дані: Дерево у вигляді вкладеного словника: { "feature": <індекс ознаки>, "value": <поріг розбиття>, "left": <ліве піддерево>, "right": <праве піддерево> } 

4. Передбачення для одного зразка

def predict_sample(tree, sample): ... 

• Мета: Передбачити клас для одного зразка.

• Логіка: 

• Рекурсивно переходити по вузлах дерева, поки не досягнуто листа.

• Вибір шляху залежить від значення ознаки та порогу розбиття.

5. Передбачення для всіх зразків

def predict(tree, X): ... 

• Мета: Передбачити класи для всіх зразків у тестовому наборі.

• Логіка: 

• Для кожного зразка викликається predict_sample.

• Вихідні дані: Масив передбачених класів.

Приклад використання

# Генерація даних from sklearn.datasets import make_classification X, y = make_classification(n_samples=100, n_features=2, n_classes=2, random_state=42) # Побудова дерева tree = build_tree(X, y, max_depth=3) # Передбачення predictions = predict(tree, X) print("Tree structure:", tree) print("Predictions:", predictions) 

Особливості реалізації

• Gini impurity як критерій: 

• Використовується для оцінки якості вузла.

• Рекурсивна побудова дерева: 

• Використання словників для представлення вузлів.

• Гнучкість: 

• Підтримка обмеження глибини дерева через параметр max_depth.

Такий опис можна розмістити у README для пояснення роботи алгоритму.

Опис роботи коду для Random Forest

Цей код реалізує базову версію алгоритму Random Forest, що є ансамблевою моделлю класифікації. Він використовує побудову кількох дерев рішень і комбінує їх передбачення для покращення якості моделі. Ось детальний опис основних компонентів:

1. Клас RandomForest

class RandomForest: def __init__(self, n_estimators=100, max_depth=None, random_state=None): self.n_estimators = n_estimators self.max_depth = max_depth self.random_state = random_state self.trees = [] 

• Мета: Ініціалізація гіперпараметрів Random Forest: 

• n_estimators: кількість дерев у лісі.

• max_depth: максимальна глибина кожного дерева.

• random_state: початкове значення для генератора випадкових чисел (забезпечує відтворюваність).

• self.trees: список для зберігання створених дерев.

2. Навчання моделі (fit)

def fit(self, X, y): np.random.seed(self.random_state) for _ in range(self.n_estimators): indices = np.random.choice(len(X), len(X), replace=True) X_sample = X[indices] y_sample = y[indices] tree = build_tree(X_sample, y_sample, max_depth=self.max_depth) self.trees.append(tree) 

• Мета: Навчання моделі шляхом створення ансамблю дерев.

• Логіка:

• Встановлення початкового стану генератора випадкових чисел (np.random.seed).

• Виконання bootstrap sampling: 

• Випадкове вибіркове копіювання з поверненням для створення навчального піднабору (X_sample, y_sample).

• Створення дерева рішень для піднабору за допомогою функції build_tree.

• Додавання створеного дерева до списку self.trees.

• Примітка: Кожне дерево створюється на основі різного піднабору даних, що забезпечує різноманітність ансамблю.

3. Передбачення (predict)

def predict(self, X): predictions = np.array([predict(tree, X) for tree in self.trees]) return np.array([np.bincount(pred).argmax() for pred in predictions.T]) 

• Мета: Обчислення передбачень для нового набору даних.

• Логіка:

• Зібрати передбачення від кожного дерева: 

• Виклик функції predict для кожного дерева на всіх зразках (X).

• Результати зберігаються в масиві predictions розміром (n_estimators, n_samples).

• Для кожного зразка виконується голосування: 

• Вибирається клас, який найчастіше передбачався деревами (модальне значення) за допомогою np.bincount.

• Вихідні дані: Масив передбачених класів розміром (n_samples,).

Особливості реалізації Random Forest

• Bootstrap Aggregating (Bagging): 

• Для кожного дерева використовується випадковий піднабір даних, що зменшує переобучення та підвищує стійкість моделі.

• Голосування: 

• Поєднання передбачень дерев за принципом більшості голосів знижує похибку моделі.

• Гнучкість: 

• Підтримка налаштування кількості дерев (n_estimators) та глибини дерева (max_depth).

Приклад використання

# Генерація даних from sklearn.datasets import make_classification X, y = make_classification(n_samples=200, n_features=4, n_classes=2, random_state=42) # Створення та навчання Random Forest rf = RandomForest(n_estimators=10, max_depth=5, random_state=42) rf.fit(X, y) # Передбачення predictions = rf.predict(X) print("Predictions:", predictions) 

Сильні сторони Random Forest

• Стійкість до шуму: Різноманітність дерев знижує ймовірність переобучення.

• Відтворюваність: Використання random_state забезпечує однакові результати при повторних запусках.

• Гнучкість у налаштуваннях: Можливість змінювати кількість дерев, їх глибину та інші параметри.
